{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9642, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.regularizers import l1\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.initializers import Initializer\n",
    "import re \n",
    "# from gensim.scripts.glove2word2vec import glove2word2ve\n",
    "\n",
    "# Load the training dataset\n",
    "df_training = pd.read_csv('C:\\\\Users\\\\ryanm\\\\OneDrive\\\\Desktop\\\\deep learning\\\\assignments\\\\Deep-Learning-Assignment-2\\\\train.csv', delimiter=',')\n",
    "X_train = df_training.values[:, 0]\n",
    "y_train = np.array(df_training.values[:, 1], dtype=np.float32)\n",
    "\n",
    "# Load the testing \n",
    "df_testing = pd.read_csv('C:\\\\Users\\\\ryanm\\\\OneDrive\\\\Desktop\\\\deep learning\\\\assignments\\\\Deep-Learning-Assignment-2\\\\test.csv')  \n",
    "X_test = np.array(df_testing.values[:, 0])\n",
    "y_test = np.array(df_testing.values[:, 1], dtype=np.float32)\n",
    "\n",
    "# Load the GLoVe word embeddings\n",
    "filename = \"C:\\\\Users\\\\ryanm\\\\OneDrive\\\\Desktop\\\\deep learning\\\\assignments\\\\Deep-Learning-Assignment-2\\\\glove.6B.300d.txt.word2vec\"\n",
    "glove = KeyedVectors.load_word2vec_format(filename, binary=False) # for now binary is true\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "num_words = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "max_len = max([len(x) for x in X_train])  # Max length of sequences\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "\n",
    "# Pre-process test data\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Construct the model weight matrix for embedding layer\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove.key_to_index:\n",
    "        embedding_vector = glove[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m he_uniform\n\u001b[1;32m----> 4\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mvocab_size()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m Sequential()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'vocab_size'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.initializers import he_uniform\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Build the model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(num_words, vocab_size))\n",
    "model_2.add(Flatten())  # Specify input shape here\n",
    "model_2.add(Dense(10, activation='relu', kernel_initializer=he_uniform(), kernel_regularizer=l1(0.001)))\n",
    "model_2.add(Dense(1, activation='sigmoid', kernel_regularizer=l1(0.001)))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history = model_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32, verbose=2)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray_equal(embedding_matrix[\u001b[38;5;241m1\u001b[39m] ,glove\u001b[38;5;241m.\u001b[39mindex_to_key(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
